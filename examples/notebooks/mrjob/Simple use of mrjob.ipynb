{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##mrjob##\n",
    "\n",
    "__mrjob__ is a software package developed by the restaurant recommendation company _Yelp_. \n",
    "It's goal is to simplify the deployment of map-reduce jobs based on streaming and python onto different \n",
    "frameworks such as Hadoop on a private cluster or hadoop on AWS (called EMR).\n",
    "\n",
    "* You can read more about mrjob here: https://pythonhosted.org/mrjob/index.html  \n",
    "* and you can clone it from github here: https://github.com/yelp/mrjob\n",
    "\n",
    "In this notebook we run a simple word-count example, add to it some logging commands, and look at two modes of running the job.\n",
    "\n",
    "**mrjob Command line** is described here: https://pythonhosted.org/mrjob/guides/emr-tools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "from ucsd_bigdata.credentials import Credentials\n",
    "\n",
    "root_dir = \"../../\"\n",
    "\n",
    "# Get the AWS credentials from the User's Vault\n",
    "credentials = Credentials()\n",
    "key_id = credentials.aws_access_key_id\n",
    "secret_key = credentials.aws_secret_access_key\n",
    "username = credentials.aws_user_name\n",
    "s3_bucket = credentials.s3_bucket\n",
    "\n",
    "print s3_bucket,key_id,username\n",
    "\n",
    "examples_dir = root_dir + '/data/text/'\n",
    "!ls -l $examples_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different modes of running a mrjob map-reduce job ##\n",
    "\n",
    "Once the mapper, combiner and reducer have been written and tested, you can run the job on different types of infrastructure:\n",
    "\n",
    "1. __inline__ run the job as a single process on the local machine.\n",
    "1. __local__ run the job on the local machine, but using multiple processes to simulate parallel processing.\n",
    "1. __EMR__ (Elastic Map Reduce) run the job on a hadoop cluster running on the amazon cloud.\n",
    "\n",
    "Below we run the same process we ran at the top using __local__ instead of the default __inline__. Observe that in this case the reducers have some non-trivial work to do even when combiners are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in local mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ucsd_bigdata.find_waiting_flow import find_waiting_flow\n",
    "flows_dict = find_waiting_flow()\n",
    "if len(flows_dict) > 0:\n",
    "    flow_id, node = (flows_dict[0]['flow_id'],flows_dict[0]['node'])\n",
    "    print flow_id, node \n",
    "    input_file = 'hdfs://'+node+':9000/weather.raw_data/ALL.csv'\n",
    "else:\n",
    "    print \"No flows available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in EMR mode on existing job flow (hadoop cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create unique output directory in the student's s3_bucket\n",
    "output_dir = s3_bucket + str(uuid.uuid4()) + \"/\"\n",
    "\n",
    "print output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python mr_word_freq_count.py -r emr  $examples_dir/Moby-Dick.txt --emr-job-flow-id=$flow_id --output-dir=$output_dir  > counts_emr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mrjob fetch-logs --list $flow_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wc counts_emr.txt\n",
    "!cat counts_emr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cut -b 2-11 counts_emr.txt > counts_only.txt\n",
    "!head -100 counts_only.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
